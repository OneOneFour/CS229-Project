{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import xarray \n",
    "import numpy as np\n",
    "ds = util.get_dataset()\n",
    "sst_ds = xarray.open_dataset(\"../data/sst.wkmean.1990-present.nc\")\n",
    "sst_ds = sst_ds.assign_coords(lon=(((sst_ds.lon + 180) % 360) - 180)) ## Use consistent longitude\n",
    "train_storms,valid_storms,test_storms = util.train_validation_test(ds,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def coriolis_parameter(lat):\n",
    "    return 2*np.sin(np.deg2rad(lat))\n",
    "\n",
    "def make_X_y(ds,selected_storms,timesteps=5):\n",
    "    Xout = []\n",
    "    yout =[]\n",
    "    storms = []\n",
    "    for storm in selected_storms:\n",
    "        usa_pres = ds.usa_pres.loc[storm]\n",
    "        usa_wind = ds.usa_wind.loc[storm]\n",
    "        ## All enteries have 360 points.\n",
    "        valid_coords = ~(np.isnan(usa_wind) | np.isnan(usa_pres))\n",
    "        lat = ds.lat.loc[storm][valid_coords]\n",
    "        lon = ds.lon.loc[storm][valid_coords]\n",
    "        storm_speed = ds.storm_speed.loc[storm][valid_coords]\n",
    "        storm_dir = ds.storm_dir.loc[storm][valid_coords]\n",
    "        usa_pres = usa_pres[valid_coords]\n",
    "        usa_wind = usa_wind[valid_coords]\n",
    "        time = ds.time.loc[storm][valid_coords]\n",
    "        cor_param = coriolis_parameter(lat)\n",
    "        try:\n",
    "            sst = sst_ds.sst.interp(time=time,lat=lat,lon=lon)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if np.isnan(sst).any():\n",
    "            continue\n",
    "\n",
    "        X = np.transpose(np.array([usa_wind,usa_pres,storm_speed,storm_dir,cor_param,sst,lat,lon]))\n",
    "        for i in range(0,len(usa_wind)):\n",
    "            if i+timesteps+1>=len(usa_wind):\n",
    "                break\n",
    "            Xout.append(X[i:i+timesteps])\n",
    "            yout.append(X[i+timesteps+1][:-4])\n",
    "            storms.append(storm)\n",
    "    return np.stack(Xout),np.stack(yout),np.array(storm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEPOINTS = 10\n",
    "x_train,y_train,storm_train = make_X_y(ds,train_storms,TIMEPOINTS)\n",
    "x_valid,y_valid,storm_valid = make_X_y(ds,valid_storms,TIMEPOINTS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=150,n_jobs=-1)\n",
    "rf.fit(x_train.reshape((x_train.shape[0],-1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 80 features, but RandomForestRegressor is expecting 40 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [79], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m pred_valid \u001b[39m=\u001b[39m rf\u001b[39m.\u001b[39;49mpredict(x_valid\u001b[39m.\u001b[39;49mreshape((x_valid\u001b[39m.\u001b[39;49mshape[\u001b[39m0\u001b[39;49m],\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)))\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:971\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    969\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    970\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[1;32m--> 971\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[0;32m    973\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    974\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:579\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[39mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    580\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[0;32m    581\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    587\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 80 features, but RandomForestRegressor is expecting 40 features as input."
     ]
    }
   ],
   "source": [
    "pred_valid = rf.predict(x_valid.reshape((x_valid.shape[0],-1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5.33147447,  4.12575757,  3.14339086, 55.67817419])"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_valid,pred_valid,multioutput='raw_values',squared=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### cross validation train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GroupKFold,cross_val_score\n",
    "cv = GroupKFold(n_splits=5)\n",
    "cross_val_score(rf,x_train.reshape((x_train.shape[0],-1)),y_train,groups=storm_train,cv=cv,scoring='neg_root_mean_squared_error',n_jobs=-1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "732e0b614408a87036c6c94cd8c4a28ec9f4a21fcd18cd6fea5bab582ca55e9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
