{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import util\n",
    "import xarray \n",
    "import numpy as np\n",
    "ds = util.get_dataset()\n",
    "sst_ds = xarray.open_dataset(\"../data/sst.wkmean.1990-present.nc\")\n",
    "sst_ds = sst_ds.assign_coords(lon=(((sst_ds.lon + 180) % 360) - 180)) ## Use consistent longitude\n",
    "train_storms,valid_storms,test_storms = util.train_validation_test(ds,seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "\n",
    "def coriolis_parameter(lat):\n",
    "    return 2*np.sin(np.deg2rad(lat))\n",
    "\n",
    "def make_X_y(ds,selected_storms,timesteps=5):\n",
    "    Xout = []\n",
    "    yout =[]\n",
    "    storms = []\n",
    "    for stormidx,storm in enumerate(selected_storms):\n",
    "        usa_pres = ds.usa_pres.loc[storm]\n",
    "        usa_wind = ds.usa_wind.loc[storm]\n",
    "        ## All enteries have 360 points.\n",
    "        valid_coords = ~(np.isnan(usa_wind) | np.isnan(usa_pres))\n",
    "        lat = ds.lat.loc[storm][valid_coords]\n",
    "        lon = ds.lon.loc[storm][valid_coords]\n",
    "        storm_speed = ds.storm_speed.loc[storm][valid_coords]\n",
    "        storm_dir = ds.storm_dir.loc[storm][valid_coords]\n",
    "        u = storm_speed*np.sin(np.deg2rad(storm_dir))\n",
    "        v = storm_speed*np.cos(np.deg2rad(storm_dir))\n",
    "\n",
    "\n",
    "        usa_pres = usa_pres[valid_coords]\n",
    "        usa_wind = usa_wind[valid_coords]\n",
    "        time = ds.time.loc[storm][valid_coords]\n",
    "        cor_param = coriolis_parameter(lat)\n",
    "        try:\n",
    "            sst = sst_ds.sst.interp(time=time,lat=lat,lon=lon)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if np.isnan(sst).any():\n",
    "            continue\n",
    "\n",
    "        X = np.transpose(np.array([usa_wind,usa_pres,u,v,sst,lat,lon]))\n",
    "        for i in range(0,len(usa_wind)):\n",
    "            if i+timesteps+1>=len(usa_wind):\n",
    "                break\n",
    "            Xout.append(X[i:i+timesteps])\n",
    "            yout.append(X[i+timesteps+1][:4])\n",
    "            storms.append(stormidx)\n",
    "    return np.stack(Xout),np.stack(yout),np.array(storms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "TIMEPOINTS = 3\n",
    "x_train,y_train,storm_train = make_X_y(ds,train_storms,TIMEPOINTS)\n",
    "x_valid,y_valid,storm_valid = make_X_y(ds,valid_storms,TIMEPOINTS)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestRegressor(max_depth=10, n_estimators=500, n_jobs=-1)"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rf = RandomForestRegressor(n_estimators=500,max_depth=10,n_jobs=-1)\n",
    "rf.fit(x_train.reshape((x_train.shape[0],-1)),y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_valid = rf.predict(x_valid.reshape((x_valid.shape[0],-1)))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.67586449, 3.76420686, 3.21790823, 3.13347694])"
      ]
     },
     "execution_count": 149,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "mean_squared_error(y_valid,pred_valid,multioutput='raw_values',squared=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tune Timepoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Timepoints 1 ---\n",
      "[5.35332694 4.09920717 3.0206419  2.92950528]\n",
      "--- Timepoints 3 ---\n",
      "[4.67110725 3.75929915 3.21514095 3.13348008]\n",
      "--- Timepoints 5 ---\n",
      "[4.75908144 3.82082262 3.24682721 3.16460579]\n",
      "--- Timepoints 10 ---\n",
      "[4.85333398 3.91229797 3.36841556 3.22959938]\n",
      "--- Timepoints 15 ---\n",
      "[4.94124005 4.05038669 3.43011289 3.29702668]\n",
      "--- Timepoints 20 ---\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [145], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[39mfor\u001b[39;00m timepoints \u001b[39min\u001b[39;00m [\u001b[39m1\u001b[39m,\u001b[39m3\u001b[39m,\u001b[39m5\u001b[39m,\u001b[39m10\u001b[39m,\u001b[39m15\u001b[39m,\u001b[39m20\u001b[39m,\u001b[39m50\u001b[39m]:\n\u001b[0;32m      3\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m--- Timepoints \u001b[39m\u001b[39m{\u001b[39;00mtimepoints\u001b[39m}\u001b[39;00m\u001b[39m ---\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m----> 4\u001b[0m     x_train_i,y_train_i,storm_train_i \u001b[39m=\u001b[39m make_X_y(ds,train_storms,timepoints)\n\u001b[0;32m      5\u001b[0m     x_valid_i,y_valid_i,storm_valid_i \u001b[39m=\u001b[39m make_X_y(ds,valid_storms,timepoints)\n\u001b[0;32m      7\u001b[0m     rf \u001b[39m=\u001b[39m RandomForestRegressor(n_estimators\u001b[39m=\u001b[39m\u001b[39m500\u001b[39m,max_depth\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m,n_jobs\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n",
      "Cell \u001b[1;32mIn [144], line 25\u001b[0m, in \u001b[0;36mmake_X_y\u001b[1;34m(ds, selected_storms, timesteps)\u001b[0m\n\u001b[0;32m     23\u001b[0m usa_pres \u001b[39m=\u001b[39m usa_pres[valid_coords]\n\u001b[0;32m     24\u001b[0m usa_wind \u001b[39m=\u001b[39m usa_wind[valid_coords]\n\u001b[1;32m---> 25\u001b[0m time \u001b[39m=\u001b[39m ds\u001b[39m.\u001b[39;49mtime\u001b[39m.\u001b[39;49mloc[storm][valid_coords]\n\u001b[0;32m     26\u001b[0m cor_param \u001b[39m=\u001b[39m coriolis_parameter(lat)\n\u001b[0;32m     27\u001b[0m \u001b[39mtry\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\dataarray.py:818\u001b[0m, in \u001b[0;36mDataArray.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    815\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_getitem_coord(key)\n\u001b[0;32m    816\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    817\u001b[0m     \u001b[39m# xarray-style array indexing\u001b[39;00m\n\u001b[1;32m--> 818\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49misel(indexers\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_item_key_to_dict(key))\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\dataarray.py:1389\u001b[0m, in \u001b[0;36mDataArray.isel\u001b[1;34m(self, indexers, drop, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[0;32m   1386\u001b[0m indexers \u001b[39m=\u001b[39m either_dict_or_kwargs(indexers, indexers_kwargs, \u001b[39m\"\u001b[39m\u001b[39misel\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m   1388\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39many\u001b[39m(is_fancy_indexer(idx) \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m indexers\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m-> 1389\u001b[0m     ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_to_temp_dataset()\u001b[39m.\u001b[39;49m_isel_fancy(\n\u001b[0;32m   1390\u001b[0m         indexers, drop\u001b[39m=\u001b[39;49mdrop, missing_dims\u001b[39m=\u001b[39;49mmissing_dims\n\u001b[0;32m   1391\u001b[0m     )\n\u001b[0;32m   1392\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_from_temp_dataset(ds)\n\u001b[0;32m   1394\u001b[0m \u001b[39m# Much faster algorithm for when all indexers are ints, slices, one-dimensional\u001b[39;00m\n\u001b[0;32m   1395\u001b[0m \u001b[39m# lists, or zero or one-dimensional np.ndarray's\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\dataset.py:2464\u001b[0m, in \u001b[0;36mDataset._isel_fancy\u001b[1;34m(self, indexers, drop, missing_dims)\u001b[0m\n\u001b[0;32m   2460\u001b[0m var_indexers \u001b[39m=\u001b[39m {\n\u001b[0;32m   2461\u001b[0m     k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m valid_indexers\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m var\u001b[39m.\u001b[39mdims\n\u001b[0;32m   2462\u001b[0m }\n\u001b[0;32m   2463\u001b[0m \u001b[39mif\u001b[39;00m var_indexers:\n\u001b[1;32m-> 2464\u001b[0m     new_var \u001b[39m=\u001b[39m var\u001b[39m.\u001b[39;49misel(indexers\u001b[39m=\u001b[39;49mvar_indexers)\n\u001b[0;32m   2465\u001b[0m     \u001b[39m# drop scalar coordinates\u001b[39;00m\n\u001b[0;32m   2466\u001b[0m     \u001b[39m# https://github.com/pydata/xarray/issues/6554\u001b[39;00m\n\u001b[0;32m   2467\u001b[0m     \u001b[39mif\u001b[39;00m name \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcoords \u001b[39mand\u001b[39;00m drop \u001b[39mand\u001b[39;00m new_var\u001b[39m.\u001b[39mndim \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\variable.py:1322\u001b[0m, in \u001b[0;36mVariable.isel\u001b[1;34m(self, indexers, missing_dims, **indexers_kwargs)\u001b[0m\n\u001b[0;32m   1319\u001b[0m indexers \u001b[39m=\u001b[39m drop_dims_from_indexers(indexers, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdims, missing_dims)\n\u001b[0;32m   1321\u001b[0m key \u001b[39m=\u001b[39m \u001b[39mtuple\u001b[39m(indexers\u001b[39m.\u001b[39mget(dim, \u001b[39mslice\u001b[39m(\u001b[39mNone\u001b[39;00m)) \u001b[39mfor\u001b[39;00m dim \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdims)\n\u001b[1;32m-> 1322\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m[key]\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\variable.py:869\u001b[0m, in \u001b[0;36mVariable.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    856\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__getitem__\u001b[39m(\u001b[39mself\u001b[39m: T_Variable, key) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m T_Variable:\n\u001b[0;32m    857\u001b[0m     \u001b[39m\"\"\"Return a new Variable object whose contents are consistent with\u001b[39;00m\n\u001b[0;32m    858\u001b[0m \u001b[39m    getting the provided key from the underlying data.\u001b[39;00m\n\u001b[0;32m    859\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    867\u001b[0m \u001b[39m    array `x.values` directly.\u001b[39;00m\n\u001b[0;32m    868\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m     dims, indexer, new_order \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_broadcast_indexes(key)\n\u001b[0;32m    870\u001b[0m     data \u001b[39m=\u001b[39m as_indexable(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data)[indexer]\n\u001b[0;32m    871\u001b[0m     \u001b[39mif\u001b[39;00m new_order:\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\variable.py:730\u001b[0m, in \u001b[0;36mVariable._broadcast_indexes\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    728\u001b[0m         dims\u001b[39m.\u001b[39mappend(d)\n\u001b[0;32m    729\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mset\u001b[39m(dims)) \u001b[39m==\u001b[39m \u001b[39mlen\u001b[39m(dims):\n\u001b[1;32m--> 730\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_broadcast_indexes_outer(key)\n\u001b[0;32m    732\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_broadcast_indexes_vectorized(key)\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\variable.py:781\u001b[0m, in \u001b[0;36mVariable._broadcast_indexes_outer\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    779\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m key:\n\u001b[0;32m    780\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(k, Variable):\n\u001b[1;32m--> 781\u001b[0m         k \u001b[39m=\u001b[39m k\u001b[39m.\u001b[39;49mdata\n\u001b[0;32m    782\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(k, BASIC_INDEXING_TYPES):\n\u001b[0;32m    783\u001b[0m         k \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39masarray(k)\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\variable.py:432\u001b[0m, in \u001b[0;36mVariable.data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    420\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[0;32m    421\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdata\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Any:\n\u001b[0;32m    422\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    423\u001b[0m \u001b[39m    The Variable's data as an array. The underlying array type\u001b[39;00m\n\u001b[0;32m    424\u001b[0m \u001b[39m    (e.g. dask, sparse, pint) is preserved.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    430\u001b[0m \u001b[39m    Variable.values\u001b[39;00m\n\u001b[0;32m    431\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 432\u001b[0m     \u001b[39mif\u001b[39;00m is_duck_array(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_data):\n\u001b[0;32m    433\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data\n\u001b[0;32m    434\u001b[0m     \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\utils.py:253\u001b[0m, in \u001b[0;36mis_duck_array\u001b[1;34m(value)\u001b[0m\n\u001b[0;32m    252\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_duck_array\u001b[39m(value: Any) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[1;32m--> 253\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39;49m(value, np\u001b[39m.\u001b[39mndarray):\n\u001b[0;32m    254\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n\u001b[0;32m    255\u001b[0m     \u001b[39mreturn\u001b[39;00m (\n\u001b[0;32m    256\u001b[0m         \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39mndim\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m    257\u001b[0m         \u001b[39mand\u001b[39;00m \u001b[39mhasattr\u001b[39m(value, \u001b[39m\"\u001b[39m\u001b[39mshape\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    262\u001b[0m         )\n\u001b[0;32m    263\u001b[0m     )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "for timepoints in [1,3,5,10,15,20,50]:\n",
    "    print(f\"--- Timepoints {timepoints} ---\")\n",
    "    x_train_i,y_train_i,storm_train_i = make_X_y(ds,train_storms,timepoints)\n",
    "    x_valid_i,y_valid_i,storm_valid_i = make_X_y(ds,valid_storms,timepoints)\n",
    "\n",
    "    rf = RandomForestRegressor(n_estimators=500,max_depth=10,n_jobs=-1)\n",
    "    rf.fit(x_train_i.reshape((x_train_i.shape[0],-1)),y_train_i)\n",
    "    y_pred_i= rf.predict(x_valid_i.reshape((x_valid_i.shape[0],-1)))\n",
    "    print(mean_squared_error(y_valid_i,y_pred_i,multioutput='raw_values',squared=False))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross Val Tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=GroupKFold(n_splits=10),\n",
       "             estimator=RandomForestRegressor(n_jobs=-1),\n",
       "             param_grid={'max_depth': [2, 5, 10],\n",
       "                         'n_estimators': [100, 250, 500]})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV,GroupKFold\n",
    "cv = GroupKFold(n_splits=10)\n",
    "hp_search_rf = RandomForestRegressor(n_jobs=-1)\n",
    "search = GridSearchCV(hp_search_rf,{\"n_estimators\":[100,250,500],\"max_depth\":[2,5,10]},cv=cv)\n",
    "search.fit(x_train.reshape((x_train.shape[0],-1)),y_train,groups=storm_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_optim = search.best_estimator_"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prediction on full test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4.29906985, 3.39276147, 2.96630253, 2.96854138])"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import mean_squared_error\n",
    "X_test,y_test,group_test = make_X_y(ds,test_storms,TIMEPOINTS)\n",
    "y_test_pred = rf.predict(X_test.reshape((X_test.shape[0],-1)))\n",
    "\n",
    "mean_squared_error(y_test,y_test_pred,multioutput='raw_values',squared=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "from geopy.distance import distance\n",
    "from geopy.point import Point\n",
    "from datetime import timedelta\n",
    "def get_predicted_latlon_from_speed(lat,lon,u,v):\n",
    "    d = np.sqrt(u**2 + v**2)*3*1.852 ## convert to kilometers across 6 hours intervals\n",
    "    bearing = np.rad2deg(np.arctan2(u,v))\n",
    "    predicted = distance(kilometers=d).destination(Point(latitude=lat,longitude=lon),bearing)\n",
    "    return predicted.latitude,predicted.longitude\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def recursive_prediction(initial_x,model,length,start_time):\n",
    "    y = np.zeros((length,6))\n",
    "    timestep = initial_x.shape[0]\n",
    "    y[:timestep] = initial_x[:,[0,1,2,3,6,7]]\n",
    "    x = initial_x\n",
    "    for step in range(length - timestep):\n",
    "        out = model.predict(x.reshape(1,-1))[0]\n",
    "        uMax,Pmin,u,v = out\n",
    "        old_lat,old_lon = x[-1,-2],x[-1,-1]\n",
    "        interp_lat,interp_lon = get_predicted_latlon_from_speed(old_lat,old_lon,u,v)\n",
    "\n",
    "        cor_param= coriolis_parameter(interp_lat)\n",
    "        sst = float(sst_ds.sst.interp(time=start_time+np.timedelta64((3)*(step+1),'h'),lat=interp_lat,lon=interp_lon))\n",
    "        if np.isnan(sst):\n",
    "            sst = x[-1,5] ## use last value if not possible\n",
    "        y[step+timestep] = [uMax,Pmin,u,v,interp_lat,interp_lon]\n",
    "        x = np.roll(x,-1,axis=0)\n",
    "        x[-1] = [uMax,Pmin,u,v,cor_param,sst,interp_lat,interp_lon]\n",
    "    return y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_storm_seeds(ds,selected_storms,timestep:int=5):\n",
    "    \"\"\"\n",
    "    Similar but generates X, vector of storm seed and y, vector of entire storm prediction track\n",
    "    \"\"\"\n",
    "    Xout,yout,start_times=[],[],[]\n",
    "\n",
    "    for storm in selected_storms:\n",
    "        usa_pres = ds.usa_pres.loc[storm]\n",
    "        usa_wind = ds.usa_wind.loc[storm]\n",
    "        ## All enteries have 360 points.\n",
    "        valid_coords = ~(np.isnan(usa_wind) | np.isnan(usa_pres))\n",
    "        lat = ds.lat.loc[storm][valid_coords]\n",
    "        lon = ds.lon.loc[storm][valid_coords]\n",
    "        storm_speed = ds.storm_speed.loc[storm][valid_coords]\n",
    "        storm_dir = ds.storm_dir.loc[storm][valid_coords]\n",
    "        u = storm_speed*np.sin(np.deg2rad(storm_dir))\n",
    "        v = storm_speed*np.cos(np.deg2rad(storm_dir))\n",
    "        usa_pres = usa_pres[valid_coords]\n",
    "        usa_wind = usa_wind[valid_coords]\n",
    "        time = ds.time.loc[storm][valid_coords]\n",
    "        cor_param = coriolis_parameter(lat)\n",
    "        try:\n",
    "            sst = sst_ds.sst.interp(time=time,lat=lat,lon=lon)\n",
    "        except ValueError:\n",
    "            continue\n",
    "        if np.isnan(sst).any():\n",
    "            continue\n",
    "        if timestep+1>=len(usa_wind):\n",
    "            continue\n",
    "\n",
    "        X = np.transpose(np.array([usa_wind,usa_pres,u,v,cor_param,sst,lat,lon]))\n",
    "    \n",
    "        Xout.append(X[:timestep])\n",
    "        yout.append(X)\n",
    "        start_times.append(time[timestep].values)\n",
    "    return Xout,yout,np.array(start_times)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_storm_seed,test_tracks,test_start_times = get_storm_seeds(ds,test_storms,timestep=TIMEPOINTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X has 24 features, but RandomForestRegressor is expecting 21 features as input.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [153], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m STORM \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m\n\u001b[0;32m      3\u001b[0m prediction_length \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(test_tracks[STORM]) \n\u001b[1;32m----> 4\u001b[0m storm_prediction \u001b[39m=\u001b[39m recursive_prediction(test_storm_seed[STORM],rf,prediction_length,test_start_times[STORM])\n\u001b[0;32m      5\u001b[0m t\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39marange(\u001b[39m0\u001b[39m,prediction_length\u001b[39m/\u001b[39m\u001b[39m8\u001b[39m,\u001b[39m0.125\u001b[39m)\n\u001b[0;32m      6\u001b[0m fig,ax \u001b[39m=\u001b[39m plt\u001b[39m.\u001b[39msubplots(ncols\u001b[39m=\u001b[39m\u001b[39m3\u001b[39m,nrows\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m,figsize\u001b[39m=\u001b[39m(\u001b[39m12\u001b[39m,\u001b[39m3\u001b[39m))\n",
      "Cell \u001b[1;32mIn [150], line 19\u001b[0m, in \u001b[0;36mrecursive_prediction\u001b[1;34m(initial_x, model, length, start_time)\u001b[0m\n\u001b[0;32m     17\u001b[0m x \u001b[39m=\u001b[39m initial_x\n\u001b[0;32m     18\u001b[0m \u001b[39mfor\u001b[39;00m step \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(length \u001b[39m-\u001b[39m timestep):\n\u001b[1;32m---> 19\u001b[0m     out \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mpredict(x\u001b[39m.\u001b[39;49mreshape(\u001b[39m1\u001b[39;49m,\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m))[\u001b[39m0\u001b[39m]\n\u001b[0;32m     20\u001b[0m     uMax,Pmin,u,v \u001b[39m=\u001b[39m out\n\u001b[0;32m     21\u001b[0m     old_lat,old_lon \u001b[39m=\u001b[39m x[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m],x[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:971\u001b[0m, in \u001b[0;36mForestRegressor.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    969\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[0;32m    970\u001b[0m \u001b[39m# Check data\u001b[39;00m\n\u001b[1;32m--> 971\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_X_predict(X)\n\u001b[0;32m    973\u001b[0m \u001b[39m# Assign chunk of trees to jobs\u001b[39;00m\n\u001b[0;32m    974\u001b[0m n_jobs, _, _ \u001b[39m=\u001b[39m _partition_estimators(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_estimators, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_jobs)\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\sklearn\\ensemble\\_forest.py:579\u001b[0m, in \u001b[0;36mBaseForest._validate_X_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    576\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    577\u001b[0m \u001b[39mValidate X whenever one tries to predict, apply, predict_proba.\"\"\"\u001b[39;00m\n\u001b[0;32m    578\u001b[0m check_is_fitted(\u001b[39mself\u001b[39m)\n\u001b[1;32m--> 579\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_validate_data(X, dtype\u001b[39m=\u001b[39;49mDTYPE, accept_sparse\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mcsr\u001b[39;49m\u001b[39m\"\u001b[39;49m, reset\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m)\n\u001b[0;32m    580\u001b[0m \u001b[39mif\u001b[39;00m issparse(X) \u001b[39mand\u001b[39;00m (X\u001b[39m.\u001b[39mindices\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc \u001b[39mor\u001b[39;00m X\u001b[39m.\u001b[39mindptr\u001b[39m.\u001b[39mdtype \u001b[39m!=\u001b[39m np\u001b[39m.\u001b[39mintc):\n\u001b[0;32m    581\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mNo support for np.int64 index based sparse matrices\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\sklearn\\base.py:585\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[1;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[0;32m    582\u001b[0m     out \u001b[39m=\u001b[39m X, y\n\u001b[0;32m    584\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m no_val_X \u001b[39mand\u001b[39;00m check_params\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mensure_2d\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mTrue\u001b[39;00m):\n\u001b[1;32m--> 585\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_check_n_features(X, reset\u001b[39m=\u001b[39;49mreset)\n\u001b[0;32m    587\u001b[0m \u001b[39mreturn\u001b[39;00m out\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\sklearn\\base.py:400\u001b[0m, in \u001b[0;36mBaseEstimator._check_n_features\u001b[1;34m(self, X, reset)\u001b[0m\n\u001b[0;32m    397\u001b[0m     \u001b[39mreturn\u001b[39;00m\n\u001b[0;32m    399\u001b[0m \u001b[39mif\u001b[39;00m n_features \u001b[39m!=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_:\n\u001b[1;32m--> 400\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    401\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mX has \u001b[39m\u001b[39m{\u001b[39;00mn_features\u001b[39m}\u001b[39;00m\u001b[39m features, but \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    402\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mis expecting \u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mn_features_in_\u001b[39m}\u001b[39;00m\u001b[39m features as input.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    403\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: X has 24 features, but RandomForestRegressor is expecting 21 features as input."
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "STORM = 20\n",
    "prediction_length = len(test_tracks[STORM]) \n",
    "storm_prediction = recursive_prediction(test_storm_seed[STORM],rf,prediction_length,test_start_times[STORM])\n",
    "t=np.arange(0,prediction_length/8,0.125)\n",
    "fig,ax = plt.subplots(ncols=3,nrows=1,figsize=(12,3))\n",
    "fig.tight_layout(pad=2)\n",
    "ax[0].plot(t,storm_prediction[:,0],'b--',label='Prediction')\n",
    "ax[0].plot(t[:len(test_tracks[STORM])],test_tracks[STORM][:,0],'r-',label='Truth')\n",
    "ax[0].set_xlabel(\"Time (days)\")\n",
    "ax[0].set_ylabel(\"Maximum Sustained Wind Speed (kts)\")\n",
    "ax[0].legend()\n",
    "ax[1].plot(t,storm_prediction[:,1],'b--',label='Prediction')\n",
    "ax[1].plot(t[:len(test_tracks[STORM])],test_tracks[STORM][:,1],'r-',label='Truth')\n",
    "ax[1].set_xlabel(\"Time (days)\")\n",
    "ax[1].set_ylabel(\"Minimum Central Pressure (hPa)\")\n",
    "ax[1].legend()\n",
    "ax[2].plot(storm_prediction[:,-1],storm_prediction[:,-2],'b--',label='Prediction')\n",
    "ax[2].plot(test_tracks[STORM][:,-1],test_tracks[STORM][:,-2],'r-',label='Truth')\n",
    "ax[2].set_xlabel(\"Longitude\")\n",
    "ax[2].set_ylabel(\"Latitude\")\n",
    "ax[2].legend()\n",
    "fig.savefig(f\"plots/rf_test_8_{STORM}.png\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Profile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [133], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[39m=\u001b[39m [recursive_prediction(test_storm_seed[storm],rf_optim,prediction_length,test_start_times[storm]) \u001b[39mfor\u001b[39;00m storm \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(test_storm_seed))]\n",
      "Cell \u001b[1;32mIn [133], line 1\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m predictions \u001b[39m=\u001b[39m [recursive_prediction(test_storm_seed[storm],rf_optim,prediction_length,test_start_times[storm]) \u001b[39mfor\u001b[39;00m storm \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(test_storm_seed))]\n",
      "Cell \u001b[1;32mIn [129], line 25\u001b[0m, in \u001b[0;36mrecursive_prediction\u001b[1;34m(initial_x, model, length, start_time)\u001b[0m\n\u001b[0;32m     22\u001b[0m interp_lat,interp_lon \u001b[39m=\u001b[39m get_predicted_latlon_from_speed(old_lat,old_lon,u,v)\n\u001b[0;32m     24\u001b[0m cor_param\u001b[39m=\u001b[39m coriolis_parameter(interp_lat)\n\u001b[1;32m---> 25\u001b[0m sst \u001b[39m=\u001b[39m \u001b[39mfloat\u001b[39m(sst_ds\u001b[39m.\u001b[39;49msst\u001b[39m.\u001b[39;49minterp(time\u001b[39m=\u001b[39;49mstart_time\u001b[39m+\u001b[39;49mnp\u001b[39m.\u001b[39;49mtimedelta64((\u001b[39m3\u001b[39;49m)\u001b[39m*\u001b[39;49m(step\u001b[39m+\u001b[39;49m\u001b[39m1\u001b[39;49m),\u001b[39m'\u001b[39;49m\u001b[39mh\u001b[39;49m\u001b[39m'\u001b[39;49m),lat\u001b[39m=\u001b[39;49minterp_lat,lon\u001b[39m=\u001b[39;49minterp_lon))\n\u001b[0;32m     26\u001b[0m \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39misnan(sst):\n\u001b[0;32m     27\u001b[0m     sst \u001b[39m=\u001b[39m x[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m,\u001b[39m5\u001b[39m] \u001b[39m## use last value if not possible\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\dataarray.py:2180\u001b[0m, in \u001b[0;36mDataArray.interp\u001b[1;34m(self, coords, method, assume_sorted, kwargs, **coords_kwargs)\u001b[0m\n\u001b[0;32m   2175\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype\u001b[39m.\u001b[39mkind \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muifc\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   2176\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\n\u001b[0;32m   2177\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minterp only works for a numeric type array. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   2178\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mGiven \u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m.\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdtype)\n\u001b[0;32m   2179\u001b[0m     )\n\u001b[1;32m-> 2180\u001b[0m ds \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_to_temp_dataset()\u001b[39m.\u001b[39minterp(\n\u001b[0;32m   2181\u001b[0m     coords,\n\u001b[0;32m   2182\u001b[0m     method\u001b[39m=\u001b[39mmethod,\n\u001b[0;32m   2183\u001b[0m     kwargs\u001b[39m=\u001b[39mkwargs,\n\u001b[0;32m   2184\u001b[0m     assume_sorted\u001b[39m=\u001b[39massume_sorted,\n\u001b[0;32m   2185\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mcoords_kwargs,\n\u001b[0;32m   2186\u001b[0m )\n\u001b[0;32m   2187\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_from_temp_dataset(ds)\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\dataset.py:3365\u001b[0m, in \u001b[0;36mDataset.interp\u001b[1;34m(self, coords, method, assume_sorted, kwargs, method_non_numeric, **coords_kwargs)\u001b[0m\n\u001b[0;32m   3362\u001b[0m \u001b[39mif\u001b[39;00m dtype_kind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39muifc\u001b[39m\u001b[39m\"\u001b[39m:\n\u001b[0;32m   3363\u001b[0m     \u001b[39m# For normal number types do the interpolation:\u001b[39;00m\n\u001b[0;32m   3364\u001b[0m     var_indexers \u001b[39m=\u001b[39m {k: v \u001b[39mfor\u001b[39;00m k, v \u001b[39min\u001b[39;00m use_indexers\u001b[39m.\u001b[39mitems() \u001b[39mif\u001b[39;00m k \u001b[39min\u001b[39;00m var\u001b[39m.\u001b[39mdims}\n\u001b[1;32m-> 3365\u001b[0m     variables[name] \u001b[39m=\u001b[39m missing\u001b[39m.\u001b[39minterp(var, var_indexers, method, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   3366\u001b[0m \u001b[39melif\u001b[39;00m dtype_kind \u001b[39min\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mObU\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mand\u001b[39;00m (use_indexers\u001b[39m.\u001b[39mkeys() \u001b[39m&\u001b[39m var\u001b[39m.\u001b[39mdims):\n\u001b[0;32m   3367\u001b[0m     \u001b[39m# For types that we do not understand do stepwise\u001b[39;00m\n\u001b[0;32m   3368\u001b[0m     \u001b[39m# interpolation to avoid modifying the elements.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   3371\u001b[0m     \u001b[39m# this loop there might be some duplicate code that slows it\u001b[39;00m\n\u001b[0;32m   3372\u001b[0m     \u001b[39m# down, therefore collect these signals and run it later:\u001b[39;00m\n\u001b[0;32m   3373\u001b[0m     reindex \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\missing.py:639\u001b[0m, in \u001b[0;36minterp\u001b[1;34m(var, indexes_coords, method, **kwargs)\u001b[0m\n\u001b[0;32m    637\u001b[0m original_dims \u001b[39m=\u001b[39m broadcast_dims \u001b[39m+\u001b[39m dims\n\u001b[0;32m    638\u001b[0m new_dims \u001b[39m=\u001b[39m broadcast_dims \u001b[39m+\u001b[39m \u001b[39mlist\u001b[39m(destination[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mdims)\n\u001b[1;32m--> 639\u001b[0m interped \u001b[39m=\u001b[39m interp_func(\n\u001b[0;32m    640\u001b[0m     var\u001b[39m.\u001b[39;49mtranspose(\u001b[39m*\u001b[39;49moriginal_dims)\u001b[39m.\u001b[39;49mdata, x, destination, method, kwargs\n\u001b[0;32m    641\u001b[0m )\n\u001b[0;32m    643\u001b[0m result \u001b[39m=\u001b[39m Variable(new_dims, interped, attrs\u001b[39m=\u001b[39mvar\u001b[39m.\u001b[39mattrs)\n\u001b[0;32m    645\u001b[0m \u001b[39m# dimension of the output array\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\missing.py:697\u001b[0m, in \u001b[0;36minterp_func\u001b[1;34m(var, x, new_x, method, kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    695\u001b[0m     func, kwargs \u001b[39m=\u001b[39m _get_interpolator_nd(method, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m--> 697\u001b[0m \u001b[39mif\u001b[39;00m is_duck_dask_array(var):\n\u001b[0;32m    698\u001b[0m     \u001b[39mimport\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39marray\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mda\u001b[39;00m\n\u001b[0;32m    700\u001b[0m     ndim \u001b[39m=\u001b[39m var\u001b[39m.\u001b[39mndim\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\pycompat.py:75\u001b[0m, in \u001b[0;36mis_duck_dask_array\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_duck_dask_array\u001b[39m(x):\n\u001b[1;32m---> 75\u001b[0m     \u001b[39mreturn\u001b[39;00m is_duck_array(x) \u001b[39mand\u001b[39;00m is_dask_collection(x)\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\pycompat.py:67\u001b[0m, in \u001b[0;36mis_dask_collection\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     66\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mis_dask_collection\u001b[39m(x):\n\u001b[1;32m---> 67\u001b[0m     \u001b[39mif\u001b[39;00m module_available(\u001b[39m\"\u001b[39;49m\u001b[39mdask\u001b[39;49m\u001b[39m\"\u001b[39;49m):\n\u001b[0;32m     68\u001b[0m         \u001b[39mfrom\u001b[39;00m \u001b[39mdask\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m \u001b[39mimport\u001b[39;00m is_dask_collection\n\u001b[0;32m     70\u001b[0m         \u001b[39mreturn\u001b[39;00m is_dask_collection(x)\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\site-packages\\xarray\\core\\utils.py:1009\u001b[0m, in \u001b[0;36mmodule_available\u001b[1;34m(module)\u001b[0m\n\u001b[0;32m    994\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmodule_available\u001b[39m(module: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mbool\u001b[39m:\n\u001b[0;32m    995\u001b[0m     \u001b[39m\"\"\"Checks whether a module is installed without importing it.\u001b[39;00m\n\u001b[0;32m    996\u001b[0m \n\u001b[0;32m    997\u001b[0m \u001b[39m    Use this for a lightweight check and lazy imports.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1007\u001b[0m \u001b[39m        Whether the module is installed.\u001b[39;00m\n\u001b[0;32m   1008\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1009\u001b[0m     \u001b[39mreturn\u001b[39;00m importlib\u001b[39m.\u001b[39;49mutil\u001b[39m.\u001b[39;49mfind_spec(module) \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Robert\\.conda\\envs\\cs229_project\\lib\\importlib\\util.py:103\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    101\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    102\u001b[0m         parent_path \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 103\u001b[0m     \u001b[39mreturn\u001b[39;00m _find_spec(fullname, parent_path)\n\u001b[0;32m    104\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    105\u001b[0m     module \u001b[39m=\u001b[39m sys\u001b[39m.\u001b[39mmodules[fullname]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:925\u001b[0m, in \u001b[0;36m_find_spec\u001b[1;34m(name, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1423\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1395\u001b[0m, in \u001b[0;36m_get_spec\u001b[1;34m(cls, fullname, path, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:1522\u001b[0m, in \u001b[0;36mfind_spec\u001b[1;34m(self, fullname, target)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:142\u001b[0m, in \u001b[0;36m_path_stat\u001b[1;34m(path)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "predictions = [recursive_prediction(test_storm_seed[storm],rf_optim,prediction_length,test_start_times[storm]) for storm in range(len(test_storm_seed))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs229_project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "732e0b614408a87036c6c94cd8c4a28ec9f4a21fcd18cd6fea5bab582ca55e9e"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
